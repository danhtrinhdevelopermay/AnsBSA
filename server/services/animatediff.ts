import axios from 'axios';
import fs from 'fs';
import path from 'path';

export interface AnimateDiffResult {
  success: boolean;
  videoUrl?: string;
  error?: string;
  description?: string;
}

// AnimateDiff video generation using Hugging Face Inference API
export async function generateVideoAnimateDiff(prompt: string, imageUrl?: string): Promise<AnimateDiffResult> {
  try {
    if (!process.env.HUGGINGFACE_TOKEN) {
      return {
        success: false,
        error: 'HUGGINGFACE_TOKEN not configured'
      };
    }

    console.log('ğŸ¬ Starting AnimateDiff video generation...');
    console.log(`ğŸ“ Prompt: ${prompt}`);
    if (imageUrl) {
      console.log(`ğŸ–¼ï¸ Input image: ${imageUrl}`);
    }

    // Available AnimateDiff models on Hugging Face
    const models = [
      'guoyww/animatediff-motion-adapter-v1-5-2',
      'ByteDance/AnimateDiff-Lightning',
      'ali-vilab/i2vgen-xl',
      'damo-vilab/text-to-video-ms-1.7b'
    ];

    let lastError = '';

    for (const model of models) {
      try {
        console.log(`ğŸ”„ Trying model: ${model}`);
        
        const requestData: any = {
          inputs: prompt
        };

        // If we have an image URL, include it for image-to-video
        if (imageUrl && imageUrl.startsWith('/web-builder-workspace/')) {
          // Read the local image file
          const imagePath = path.join(process.cwd(), 'web-builder-workspace', path.basename(imageUrl));
          if (fs.existsSync(imagePath)) {
            const imageBuffer = fs.readFileSync(imagePath);
            const base64Image = imageBuffer.toString('base64');
            requestData.inputs = {
              prompt: prompt,
              image: `data:image/png;base64,${base64Image}`
            };
          }
        }

        const response = await axios.post(
          `https://api-inference.huggingface.co/models/${model}`,
          requestData,
          {
            headers: {
              'Authorization': `Bearer ${process.env.HUGGINGFACE_TOKEN}`,
              'Content-Type': 'application/json'
            },
            responseType: 'arraybuffer',
            timeout: 120000 // 2 minutes timeout
          }
        );

        if (response.status === 200 && response.data.byteLength > 1000) {
          const timestamp = Date.now();
          const fileName = `animatediff_video_${timestamp}.mp4`;
          const filePath = path.join(process.cwd(), 'web-builder-workspace', fileName);
          
          await fs.promises.writeFile(filePath, Buffer.from(response.data));
          const videoUrl = `/web-builder-workspace/${fileName}`;
          
          console.log(`âœ… AnimateDiff video generated successfully with ${model}`);
          
          return {
            success: true,
            videoUrl,
            description: `ğŸ¬ **Video AnimateDiff Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng!**

âœ… **Táº¡o video tá»« prompt:** "${prompt}"${imageUrl ? `\nğŸ–¼ï¸ **Tá»« áº£nh gá»‘c:** ${imageUrl}` : ''}
ğŸ¯ **Model sá»­ dá»¥ng:** ${model}
ğŸ“ **ÄÃ£ lÆ°u vÃ o workspace:** ${videoUrl}
â±ï¸ **Thá»i gian:** ${new Date().toLocaleTimeString('vi-VN')}

ğŸ¨ **Äáº·c Ä‘iá»ƒm AnimateDiff:**
â€¢ Táº¡o video tá»« vÄƒn báº£n hoáº·c áº£nh
â€¢ Chuyá»ƒn Ä‘á»™ng tá»± nhiÃªn vÃ  mÆ°á»£t mÃ   
â€¢ Cháº¥t lÆ°á»£ng HD, thá»i lÆ°á»£ng ngáº¯n
â€¢ HoÃ n toÃ n miá»…n phÃ­ vá»›i Hugging Face

ğŸ’¡ **Tip:** Báº¡n cÃ³ thá»ƒ xem video ngay tá»« workspace hoáº·c táº£i vá»!`
          };
        }

        // Check if model is loading
        if (response.status === 503) {
          lastError = `Model ${model} is loading, please wait`;
          continue;
        }

      } catch (modelError: any) {
        console.log(`âš ï¸ Model ${model} failed:`, modelError.message);
        lastError = modelError.message;
        
        // If it's a rate limit, wait and try next model
        if (modelError.response?.status === 429) {
          console.log('Rate limited, trying next model...');
          continue;
        }
        
        // If model not found, try next
        if (modelError.response?.status === 404) {
          console.log('Model not available, trying next...');
          continue;
        }
      }
    }

    return {
      success: false,
      error: `All AnimateDiff models failed. Last error: ${lastError}`,
      description: `ğŸ¬ **AnimateDiff Video Generation**

âš ï¸ **Táº¡m thá»i khÃ´ng kháº£ dá»¥ng** - CÃ¡c model Ä‘ang Ä‘Æ°á»£c táº£i

**ğŸ”„ ÄÃ£ thá»­ cÃ¡c model:**
â€¢ guoyww/animatediff-motion-adapter-v1-5-2
â€¢ ByteDance/AnimateDiff-Lightning  
â€¢ ali-vilab/i2vgen-xl
â€¢ damo-vilab/text-to-video-ms-1.7b

**ğŸ’¡ Giáº£i phÃ¡p thay tháº¿:**
â€¢ Thá»­ láº¡i sau 1-2 phÃºt
â€¢ Sá»­ dá»¥ng tÃ­nh nÄƒng táº¡o video khÃ¡c
â€¢ Hoáº·c táº¡o áº£nh trÆ°á»›c, sau Ä‘Ã³ táº¡o video tá»« áº£nh

**ğŸ¯ AnimateDiff sáº½ hoáº¡t Ä‘á»™ng khi:**
â€¢ Model Ä‘Æ°á»£c táº£i xong trÃªn Hugging Face
â€¢ Há»‡ thá»‘ng tá»± Ä‘á»™ng thá»­ láº¡i cÃ¡c model kháº£ dá»¥ng`
    };

  } catch (error: any) {
    console.error('AnimateDiff generation error:', error);
    return {
      success: false,
      error: error.message,
      description: `âŒ **Lá»—i táº¡o video AnimateDiff**

**Chi tiáº¿t lá»—i:** ${error.message}

**ğŸ”„ Thá»­ láº¡i:**
â€¢ Kiá»ƒm tra káº¿t ná»‘i internet
â€¢ Thá»­ prompt ngáº¯n gá»n hÆ¡n
â€¢ Hoáº·c sá»­ dá»¥ng tÃ­nh nÄƒng táº¡o video khÃ¡c

**ğŸ’¡ CÃ¡c tÃ­nh nÄƒng video khÃ¡c:**
â€¢ Replicate API (náº¿u cÃ³ token)
â€¢ fal.ai (tráº£ phÃ­)
â€¢ Táº¡o áº£nh trÆ°á»›c, sau Ä‘Ã³ animate`
    };
  }
}

// Enhanced video detection that includes AnimateDiff keywords
export function detectAnimateDiffRequest(message: string): boolean {
  const animateKeywords = [
    'animatediff', 'animate diff', 'táº¡o video tá»« áº£nh', 'video tá»« áº£nh',
    'animate áº£nh', 'lÃ m áº£nh Ä‘á»™ng', 'biáº¿n áº£nh thÃ nh video',
    'image to video', 'i2v', 'áº£nh thÃ nh video', 'chuyá»ƒn áº£nh thÃ nh video'
  ];
  
  const messageText = message.toLowerCase();
  return animateKeywords.some(keyword => messageText.includes(keyword));
}

// Get setup instructions for AnimateDiff
export function getAnimateDiffSetup(): string {
  return `ğŸ¬ **AnimateDiff - Táº¡o Video tá»« áº¢nh/VÄƒn báº£n**

**ğŸ¯ TÃ­nh nÄƒng:**
â€¢ **Text-to-Video:** Táº¡o video tá»« mÃ´ táº£ vÄƒn báº£n
â€¢ **Image-to-Video:** Táº¡o video tá»« áº£nh cÃ³ sáºµn
â€¢ **Animation:** Chuyá»ƒn Ä‘á»™ng tá»± nhiÃªn, mÆ°á»£t mÃ 
â€¢ **Free:** HoÃ n toÃ n miá»…n phÃ­ vá»›i Hugging Face

**ğŸ”§ ÄÃ£ tÃ­ch há»£p sáºµn:**
âœ… Hugging Face Token Ä‘Ã£ cáº¥u hÃ¬nh
âœ… 4 model AnimateDiff kháº£ dá»¥ng
âœ… Há»— trá»£ cáº£ text-to-video vÃ  image-to-video
âœ… Tá»± Ä‘á»™ng lÆ°u video vÃ o workspace

**ğŸ’¡ CÃ¡ch sá»­ dá»¥ng:**

**1. Text-to-Video:**
â€¢ "Táº¡o video con mÃ¨o Ä‘ang cháº¡y"
â€¢ "AnimateDiff: ngÆ°á»i Ä‘ang nháº£y"

**2. Image-to-Video:**  
â€¢ Táº¡o áº£nh trÆ°á»›c: "Táº¡o áº£nh con chÃ³"
â€¢ Sau Ä‘Ã³: "Táº¡o video tá»« áº£nh nÃ y, cho nÃ³ Ä‘ang cháº¡y"

**ğŸ¨ Models kháº£ dá»¥ng:**
â€¢ ByteDance/AnimateDiff-Lightning (nhanh)
â€¢ guoyww/animatediff-motion-adapter-v1-5-2 (cháº¥t lÆ°á»£ng)
â€¢ ali-vilab/i2vgen-xl (image-to-video chuyÃªn dá»¥ng)
â€¢ damo-vilab/text-to-video-ms-1.7b (text-to-video)

HÃ£y thá»­ ngay: "Táº¡o video con mÃ¨o Ä‘ang chÆ¡i"!`;
}